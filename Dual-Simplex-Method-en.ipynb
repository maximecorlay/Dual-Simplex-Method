{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmjtizwNQkd7gqon7Y4lTk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Dual simplex method : explanations with an easy example\n","\n","### Prequisites: simplex algorithm (see Simplex-Algorithm-Linear-Programming) and branch-and-bound method (see Branch-and-Bound-Diving-Contest)\n","\n","The dual simplex method begins with a basis that has negative reduced costs (in maximization) or positive reduced costs (in minimization), but is not feasible, meaning that some slack variables are $<0$.\n","\n","For instance, let us consider the following problem:\n","\n","$$\n","\\text{Maximize } -x - y\n","$$\n","$$\n","u = 3 - x - y\n","$$\n","$$\n","v = -2 + x - y\n","$$\n","$$\n","x, y, u, v \\geq 0\n","$$\n","\n","For this problem, $(u,v)$ is clearly a basis, and it is such that the non-basic variables ($x$ and $y$) have reduced cost $\\leq 0$. We say that the basis $(u,v)$ is **dual feasible**.\n","\n","However, $(u,v)$ is not primal feasible because $v=-2$ according to the second constraint (while all slack variables must be $\\geq 0$ according to the third constraint).\n","\n","The dual simplex algorithm allows us to start from this kind of situation and reach a primal feasible optimal basis.\n","\n","**But in what kind of situation is this useful? Wouldn't it be better to directly apply the simplex algorithm?**\n","\n","* This is useful when **we have reached the optimum but want to add a constraint**. In the example above, if there was no 2nd constraint, we would have reached the optimum in the basis (composed of a single element) $(u)=(3)$, since all the reduced costs are $\\leq 0$. But adding the constraint $v = -2 + x - y$ makes this basis non primal-feasible.\n","* When you have reached the optimum and you want to add a constraint, you would normally have to start all over again: add a slack variable, enlarge the matrices, invert everything and start the simplex algo calculations all over again. Instead of doing all this, the dual simplex algo allows you to **start from the previous optimal basis** and **adjust** it with very few calculations to satisfy the new constraints.\n","\n","**What's the point of looking at problems where the optimum was reached, but where a constraint is added after the fact?**\n","\n","This is typically what is needed in Linear Integer Programming (LIP)! In the branch-and-bound algorithm, the problem is broken down into sub-problems by adding more and more constraints. Without the dual simplex algorithm, the calculations would have to be repeated for each branch. This would be very time-consuming and inefficient!\n","\n","**So, how do we solve the above problem?**\n","\n","* We start with the basis $(u,v)$, which is dual-feasible but not primal-feasible. As a reminder, it is dual-feasible because the corresponding non-basic variables ($x$ and $y$) have a reduced cost $\\leq 0$ (whereas we are in maximization).\n","* We list the base variables that prevent primal-feasability: $v$. This is the variable we are going to remove from the base. Look at the corresponding constraint: $v = -2 + x - y$. Next, calculate the minimum of the cost ratios reduced by the opposite of the coefficients of the variables outside the base **considering ONLY variables with coefficients $>0$ (which only concerns $x$ here)**: $min(\\frac{-1}{-1})=1$ so $x$ enters the base.\n","* Rotate the equation $v = -2 + x - y$, which becomes $x=2+y+v$, then inject the whole problem (constraints + objective), which gives:\n","$$\n","\\text{Maximize } -2-2y-v\n","$$\n","$$\n","u = 1 - 2y -v\n","$$\n","$$\n","x=2+y+v\n","$$\n","$$\n","x, y, u, v \\geq 0\n","$$\n","* We see that the new basis $(u,x)$ is primal-feasible (and all reduced costs are negative, so dual-feasability is verified). The optimum is thus reached, and much more quickly than if we had done all the matrix calculations of the simplex method.\n","* If this wasn't the case, we would simply continue by looking at the constraints preventing feasibility (to remove them from the base), calculating the ratios and so on.\n","\n","## To conclude\n","\n","* Despite its name, you do not need to know how to write the “dual” of the optimization problem to apply the dual simplex algorithm: this algorithm applies directly to the primal problem!\n","* This algo isn't very useful if you're starting from scratch. On the other hand, in the midst of a LIP branch-and-bound method, it is interesting to use the dual simplex as above when adding a constraint (typically “$x \\leq1$” or “$x \\geq 2$”) to a problem whose optimum is already known, as this avoids reapplying the simplex in each node and thus recalculating everything.\n","* In the simplex algo, you first enter a variable using the reduced costs (taking the one with the highest reduced cost in maximization), then output a variable by looking at the constraints. In the dual simplex algo, it's the other way round: first, we bring out a variable by looking at the constraints, and then we bring in a variable by looking at the ratios with the reduced costs.\n","\n","\n","\n"],"metadata":{"id":"zK9AWUAzUmUB"}}]}